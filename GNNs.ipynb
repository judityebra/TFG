{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a4210fe",
   "metadata": {},
   "source": [
    "## VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0247a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca7dcff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = \"dades_tfg/data\"\n",
    "demographics = pd.read_csv(os.path.join(basepath, \"demographics.csv\"))\n",
    "demographics_N = pd.read_csv(os.path.join(basepath, \"demographics_N.csv\"))\n",
    "nodes = pd.read_csv(os.path.join(basepath, \"nodes.csv\"))\n",
    "basepath_FA = os.path.join(basepath, \"FA\")\n",
    "basepath_GM = os.path.join(basepath, \"GM\")\n",
    "basepath_RS = os.path.join(basepath, \"RS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43f22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"{:04d}.csv\".format(x) for x in demographics[\"id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f31094e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_FA = np.zeros(shape=(len(filenames), 76, 76))\n",
    "data_GM = np.zeros(shape=(len(filenames), 76, 76))\n",
    "data_RS = np.zeros(shape=(len(filenames), 76, 76))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b19da5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, filename in enumerate(filenames):\n",
    "    df_FA = pd.read_csv(os.path.join(basepath_FA, filename), header=None)\n",
    "    data_FA[i,:,:] = df_FA.values\n",
    "    \n",
    "    df_GM = pd.read_csv(os.path.join(basepath_GM, filename), header=None)\n",
    "    data_GM[i,:,:] = df_GM.values\n",
    "    \n",
    "    df_RS = pd.read_csv(os.path.join(basepath_RS, filename), header=None)\n",
    "    data_RS[i,:,:] = df_RS.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d634aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath_FA_N = \"dades_tfg/data/Naples/DTI_networks\"\n",
    "basepath_GM_N = \"dades_tfg/data/Naples/GM_networks\"\n",
    "basepath_RS_N = \"dades_tfg/data/Naples/rsfmri_networks\"\n",
    "\n",
    "# Listar nombres de archivos CSV en cada carpeta\n",
    "filenames_FA_N = sorted([f for f in os.listdir(basepath_FA_N) if f.endswith('.csv')])\n",
    "filenames_GM_N = sorted([f for f in os.listdir(basepath_GM_N) if f.endswith('.csv')])\n",
    "filenames_RS_N = sorted([f for f in os.listdir(basepath_RS_N) if f.endswith('.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb46c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar arrays para almacenar los datos\n",
    "data_FA_N = np.zeros((len(filenames_FA_N), 76, 76))  \n",
    "data_GM_N = np.zeros((len(filenames_GM_N), 76, 76))\n",
    "data_RS_N = np.zeros((len(filenames_RS_N), 76, 76))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d50e42c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FA\n",
    "for i, filename in enumerate(filenames_FA_N):\n",
    "    df_FA_N = pd.read_csv(os.path.join(basepath_FA_N, filename), header=None)\n",
    "    data_FA_N[i, :, :] = df_FA_N.values\n",
    "\n",
    "#  GM\n",
    "for i, filename in enumerate(filenames_GM_N):\n",
    "    df_GM_N = pd.read_csv(os.path.join(basepath_GM_N, filename), header=None)\n",
    "    data_GM_N[i, :, :] = df_GM_N.values\n",
    "\n",
    "#  RS\n",
    "for i, filename in enumerate(filenames_RS_N):\n",
    "    df_RS_N = pd.read_csv(os.path.join(basepath_RS_N, filename), header=None)\n",
    "    data_RS_N[i, :, :] = df_RS_N.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41611546",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_FA = len(data_FA)\n",
    "num_samples_FA_N = len(data_FA_N)\n",
    "num_samples_GM = len(data_FA)\n",
    "num_samples_GM_N = len(data_FA_N)\n",
    "num_samples_RS = len(data_FA)\n",
    "num_samples_RS_N = len(data_FA_N)\n",
    "\n",
    "\n",
    "data_FA_combined = np.zeros((num_samples_FA + num_samples_FA_N, 76, 76))  \n",
    "data_FA_combined[:num_samples_FA, :, :] = data_FA\n",
    "data_FA_combined[num_samples_FA:, :, :] = data_FA_N\n",
    "\n",
    "\n",
    "data_GM_combined = np.zeros((num_samples_GM + num_samples_GM_N, 76, 76))  \n",
    "data_GM_combined[:num_samples_GM, :, :] = data_GM\n",
    "data_GM_combined[num_samples_GM:, :, :] = data_GM_N\n",
    "\n",
    "\n",
    "data_RS_combined = np.zeros((num_samples_RS + num_samples_RS_N, 76, 76))  \n",
    "data_RS_combined[:num_samples_RS, :, :] = data_RS\n",
    "data_RS_combined[num_samples_RS:, :, :] = data_RS_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4263fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_N['mstype'] = demographics_N['mstype'].apply(lambda x: 0 if x == -1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f70c57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 76, 76)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_FA_combined.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "659fe897",
   "metadata": {},
   "source": [
    "nodes_FA = [nodo + 1 for nodo in G_fa.nodes()]\n",
    "nodes_GM = [nodo + 1 for nodo in G_GM.nodes()]\n",
    "nodes_RS = [nodo + 1 for nodo in G_RS.nodes()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2809ed45",
   "metadata": {},
   "source": [
    "## GNNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c22d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# CAL DEFINIR ELS EMBEDDINGS ES FARAN SERVIR\n",
    "# EN AQUEST CODI, NOMÉS ÉS UN VECTOR DE (node_dim X 1's)\n",
    "node_dim = 1  # Asegúrate de que esto coincida con las dimensiones del archivo\n",
    "#node_dim = 76 si fem els les metriques dels nodes\n",
    "\n",
    "def array_to_graph(data, y, th=0.0):\n",
    "    num_nodes = data.shape[0]\n",
    "    \n",
    "    edge_index = []\n",
    "    edge_weight = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:\n",
    "                if data[i, j] >= th:\n",
    "                    edge_index.append([i, j])\n",
    "                    edge_weight.append(data[i, j])\n",
    "        \n",
    "    y = torch.tensor([int(y)], dtype=torch.long)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    \n",
    "    # Cargar datos de embeddings desde el archivo CSV\n",
    "    embeddings_df = pd.read_csv('csvs/average_clustering_fa.csv', header=None) #modificar csv a necessitat\n",
    "    x = torch.tensor(embeddings_df.values, dtype=torch.float)\n",
    "\n",
    "    edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(), edge_weight=edge_weight, y=y)\n",
    "    #print(x)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee6b8948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_max_pool\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(node_dim, 64)\n",
    "        self.lin1 = torch.nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_weight, data.batch\n",
    "        \n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = global_max_pool(x, batch)\n",
    "        x = self.lin1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5626799e-0d6f-43cc-8c56-47bebd9a9aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270, 76, 76)\n",
      "(270,)\n"
     ]
    }
   ],
   "source": [
    "data = data_FA_combined\n",
    "print(data.shape)\n",
    "\n",
    "target = demographics_N['mstype'].to_numpy()\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1322fa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.6903\n",
      "Train loss at epoch 2: 0.6896\n",
      "Train loss at epoch 3: 0.6896\n",
      "Train loss at epoch 4: 0.6900\n",
      "Train loss at epoch 5: 0.6893\n",
      "Train loss at epoch 6: 0.6894\n",
      "Train loss at epoch 7: 0.6892\n",
      "Train loss at epoch 8: 0.6893\n",
      "Train loss at epoch 9: 0.6892\n",
      "Train loss at epoch 10: 0.6892\n",
      "Train loss at epoch 11: 0.6892\n",
      "Train loss at epoch 12: 0.6893\n",
      "Train loss at epoch 13: 0.6892\n",
      "Train loss at epoch 14: 0.6894\n",
      "Train loss at epoch 15: 0.6892\n",
      "Train loss at epoch 16: 0.6894\n",
      "Train loss at epoch 17: 0.6893\n",
      "Train loss at epoch 18: 0.6895\n",
      "Train loss at epoch 19: 0.6893\n",
      "Train loss at epoch 20: 0.6892\n",
      "Test AUC: 0.50\n",
      "Fold: 2\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.6929\n",
      "Train loss at epoch 2: 0.6920\n",
      "Train loss at epoch 3: 0.6910\n",
      "Train loss at epoch 4: 0.6904\n",
      "Train loss at epoch 5: 0.6900\n",
      "Train loss at epoch 6: 0.6897\n",
      "Train loss at epoch 7: 0.6895\n",
      "Train loss at epoch 8: 0.6894\n",
      "Train loss at epoch 9: 0.6894\n",
      "Train loss at epoch 10: 0.6894\n",
      "Train loss at epoch 11: 0.6893\n",
      "Train loss at epoch 12: 0.6894\n",
      "Train loss at epoch 13: 0.6893\n",
      "Train loss at epoch 14: 0.6892\n",
      "Train loss at epoch 15: 0.6893\n",
      "Train loss at epoch 16: 0.6893\n",
      "Train loss at epoch 17: 0.6892\n",
      "Train loss at epoch 18: 0.6892\n",
      "Train loss at epoch 19: 0.6892\n",
      "Train loss at epoch 20: 0.6892\n",
      "Test AUC: 0.50\n",
      "Fold: 3\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.6909\n",
      "Train loss at epoch 2: 0.6900\n",
      "Train loss at epoch 3: 0.6899\n",
      "Train loss at epoch 4: 0.6897\n",
      "Train loss at epoch 5: 0.6896\n",
      "Train loss at epoch 6: 0.6894\n",
      "Train loss at epoch 7: 0.6894\n",
      "Train loss at epoch 8: 0.6893\n",
      "Train loss at epoch 9: 0.6893\n",
      "Train loss at epoch 10: 0.6892\n",
      "Train loss at epoch 11: 0.6893\n",
      "Train loss at epoch 12: 0.6892\n",
      "Train loss at epoch 13: 0.6892\n",
      "Train loss at epoch 14: 0.6892\n",
      "Train loss at epoch 15: 0.6892\n",
      "Train loss at epoch 16: 0.6892\n",
      "Train loss at epoch 17: 0.6892\n",
      "Train loss at epoch 18: 0.6892\n",
      "Train loss at epoch 19: 0.6893\n",
      "Train loss at epoch 20: 0.6892\n",
      "Test AUC: 0.50\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "preds = np.zeros(data.shape[0])\n",
    "fold = 0\n",
    "\n",
    "for train_index, test_index in skf.split(data, target):\n",
    "    fold += 1\n",
    "    print(\"Fold: {}\".format(fold))\n",
    "\n",
    "    # split dataset\n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    prop_train = np.where(y_train == 1)[0].shape[0] / y_train.shape[0]\n",
    "    prop_test = np.where(y_test == 1)[0].shape[0] / y_test.shape[0]\n",
    "    print(\"Train set size     : {}\".format(X_train.shape))\n",
    "    print(\"Test set size      : {}\".format(X_test.shape))\n",
    "    print(\"Train set % of pwMS: {:.4f} ({})\".format(prop_train, y_train.sum()))\n",
    "    print(\"Test set % of pwMS : {:.4f} ({})\".format(prop_test, y_test.sum()))\n",
    "\n",
    "    # list of Data structures (one for each subject)\n",
    "    train_graphs = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        g = array_to_graph(X_train[i], y_train[i])\n",
    "        train_graphs.append(g)\n",
    "        \n",
    "    test_graphs = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        g = array_to_graph(X_test[i], y_test[i])\n",
    "        test_graphs.append(g)\n",
    "\n",
    "    # create the model\n",
    "    model = GCN()\n",
    "    model = model.to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # train function\n",
    "    def train():\n",
    "        model.train()\n",
    "        \n",
    "        train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n",
    "\n",
    "        loss_all = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            label = batch.y\n",
    "            label = F.one_hot(label, num_classes=2)\n",
    "            label = label.type(torch.FloatTensor)\n",
    "            label = label.to(device)\n",
    "            loss = loss_fn(output, label)\n",
    "            loss.backward()\n",
    "            loss_all += batch.num_graphs * loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        return loss_all / len(train_graphs)\n",
    "\n",
    "    # train for N epochs\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        loss_value = train()\n",
    "        print(\"Train loss at epoch {}: {:.4f}\".format(epoch + 1, loss_value))\n",
    "\n",
    "    # test phase \n",
    "    test_loader = DataLoader(test_graphs, batch_size=len(test_graphs), shuffle=False)\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        test_preds = F.softmax(model(batch), dim=1).detach().numpy()\n",
    "    \n",
    "    test_preds = test_preds[:, 1]\n",
    "    preds[test_index] = test_preds\n",
    "    \n",
    "    auc_roc = roc_auc_score(y_test, test_preds)\n",
    "    print(\"Test AUC: {:.2f}\".format(auc_roc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61c2e7e2-30e5-4f13-b401-b249f2f7878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# CAL DEFINIR ELS EMBEDDINGS ES FARAN SERVIR\n",
    "# EN AQUEST CODI, NOMÉS ÉS UN VECTOR DE (node_dim X 1's)\n",
    "node_dim = 1  # Asegúrate de que esto coincida con las dimensiones del archivo\n",
    "\n",
    "def array_to_graph(data, y, th=0.0):\n",
    "    num_nodes = data.shape[0]\n",
    "    \n",
    "    edge_index = []\n",
    "    edge_weight = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:\n",
    "                if data[i, j] >= th:\n",
    "                    edge_index.append([i, j])\n",
    "                    edge_weight.append(data[i, j])\n",
    "        \n",
    "    y = torch.tensor([int(y)], dtype=torch.long)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    \n",
    "    # Cargar datos de embeddings desde el archivo CSV\n",
    "    embeddings_df = pd.read_csv('csvs/average_clustering_gm.csv', header=None) #modificar csv a necessitat\n",
    "    x = torch.tensor(embeddings_df.values, dtype=torch.float)\n",
    "\n",
    "    edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(), edge_weight=edge_weight, y=y)\n",
    "    #print(x)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e1d0bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270, 76, 76)\n",
      "(270,)\n"
     ]
    }
   ],
   "source": [
    "data = data_GM_combined\n",
    "print(data.shape)\n",
    "\n",
    "target = demographics_N['mstype'].to_numpy()\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea375284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.6869\n",
      "Train loss at epoch 2: 0.6804\n",
      "Train loss at epoch 3: 0.6772\n",
      "Train loss at epoch 4: 0.6736\n",
      "Train loss at epoch 5: 0.6707\n",
      "Train loss at epoch 6: 0.6686\n",
      "Train loss at epoch 7: 0.6666\n",
      "Train loss at epoch 8: 0.6651\n",
      "Train loss at epoch 9: 0.6631\n",
      "Train loss at epoch 10: 0.6614\n",
      "Train loss at epoch 11: 0.6595\n",
      "Train loss at epoch 12: 0.6577\n",
      "Train loss at epoch 13: 0.6558\n",
      "Train loss at epoch 14: 0.6536\n",
      "Train loss at epoch 15: 0.6516\n",
      "Train loss at epoch 16: 0.6492\n",
      "Train loss at epoch 17: 0.6468\n",
      "Train loss at epoch 18: 0.6448\n",
      "Train loss at epoch 19: 0.6415\n",
      "Train loss at epoch 20: 0.6388\n",
      "Test AUC: 0.76\n",
      "Fold: 2\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.6907\n",
      "Train loss at epoch 2: 0.6882\n",
      "Train loss at epoch 3: 0.6874\n",
      "Train loss at epoch 4: 0.6848\n",
      "Train loss at epoch 5: 0.6844\n",
      "Train loss at epoch 6: 0.6830\n",
      "Train loss at epoch 7: 0.6824\n",
      "Train loss at epoch 8: 0.6817\n",
      "Train loss at epoch 9: 0.6810\n",
      "Train loss at epoch 10: 0.6804\n",
      "Train loss at epoch 11: 0.6800\n",
      "Train loss at epoch 12: 0.6792\n",
      "Train loss at epoch 13: 0.6786\n",
      "Train loss at epoch 14: 0.6780\n",
      "Train loss at epoch 15: 0.6774\n",
      "Train loss at epoch 16: 0.6767\n",
      "Train loss at epoch 17: 0.6760\n",
      "Train loss at epoch 18: 0.6752\n",
      "Train loss at epoch 19: 0.6745\n",
      "Train loss at epoch 20: 0.6736\n",
      "Test AUC: 1.00\n",
      "Fold: 3\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.6867\n",
      "Train loss at epoch 2: 0.6845\n",
      "Train loss at epoch 3: 0.6839\n",
      "Train loss at epoch 4: 0.6818\n",
      "Train loss at epoch 5: 0.6803\n",
      "Train loss at epoch 6: 0.6794\n",
      "Train loss at epoch 7: 0.6788\n",
      "Train loss at epoch 8: 0.6773\n",
      "Train loss at epoch 9: 0.6764\n",
      "Train loss at epoch 10: 0.6756\n",
      "Train loss at epoch 11: 0.6745\n",
      "Train loss at epoch 12: 0.6734\n",
      "Train loss at epoch 13: 0.6722\n",
      "Train loss at epoch 14: 0.6712\n",
      "Train loss at epoch 15: 0.6700\n",
      "Train loss at epoch 16: 0.6689\n",
      "Train loss at epoch 17: 0.6676\n",
      "Train loss at epoch 18: 0.6662\n",
      "Train loss at epoch 19: 0.6650\n",
      "Train loss at epoch 20: 0.6632\n",
      "Test AUC: 1.00\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "preds = np.zeros(data.shape[0])\n",
    "fold = 0\n",
    "\n",
    "for train_index, test_index in skf.split(data, target):\n",
    "    fold += 1\n",
    "    print(\"Fold: {}\".format(fold))\n",
    "\n",
    "    # split dataset\n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    prop_train = np.where(y_train == 1)[0].shape[0] / y_train.shape[0]\n",
    "    prop_test = np.where(y_test == 1)[0].shape[0] / y_test.shape[0]\n",
    "    print(\"Train set size     : {}\".format(X_train.shape))\n",
    "    print(\"Test set size      : {}\".format(X_test.shape))\n",
    "    print(\"Train set % of pwMS: {:.4f} ({})\".format(prop_train, y_train.sum()))\n",
    "    print(\"Test set % of pwMS : {:.4f} ({})\".format(prop_test, y_test.sum()))\n",
    "\n",
    "    # list of Data structures (one for each subject)\n",
    "    train_graphs = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        g = array_to_graph(X_train[i], y_train[i])\n",
    "        train_graphs.append(g)\n",
    "        \n",
    "    test_graphs = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        g = array_to_graph(X_test[i], y_test[i])\n",
    "        test_graphs.append(g)\n",
    "\n",
    "    # create the model\n",
    "    model = GCN()\n",
    "    model = model.to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # train function\n",
    "    def train():\n",
    "        model.train()\n",
    "        \n",
    "        train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n",
    "\n",
    "        loss_all = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            label = batch.y\n",
    "            label = F.one_hot(label, num_classes=2)\n",
    "            label = label.type(torch.FloatTensor)\n",
    "            label = label.to(device)\n",
    "            loss = loss_fn(output, label)\n",
    "            loss.backward()\n",
    "            loss_all += batch.num_graphs * loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        return loss_all / len(train_graphs)\n",
    "\n",
    "    # train for N epochs\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        loss_value = train()\n",
    "        print(\"Train loss at epoch {}: {:.4f}\".format(epoch + 1, loss_value))\n",
    "\n",
    "    # test phase \n",
    "    test_loader = DataLoader(test_graphs, batch_size=len(test_graphs), shuffle=False)\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        test_preds = F.softmax(model(batch), dim=1).detach().numpy()\n",
    "    \n",
    "    test_preds = test_preds[:, 1]\n",
    "    preds[test_index] = test_preds\n",
    "    \n",
    "    auc_roc = roc_auc_score(y_test, test_preds)\n",
    "    print(\"Test AUC: {:.2f}\".format(auc_roc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c94d5f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# CAL DEFINIR ELS EMBEDDINGS ES FARAN SERVIR\n",
    "# EN AQUEST CODI, NOMÉS ÉS UN VECTOR DE (node_dim X 1's)\n",
    "node_dim = 1  # Asegúrate de que esto coincida con las dimensiones del archivo\n",
    "\n",
    "def array_to_graph(data, y, th=0.0):\n",
    "    num_nodes = data.shape[0]\n",
    "    \n",
    "    edge_index = []\n",
    "    edge_weight = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:\n",
    "                if data[i, j] >= th:\n",
    "                    edge_index.append([i, j])\n",
    "                    edge_weight.append(data[i, j])\n",
    "        \n",
    "    y = torch.tensor([int(y)], dtype=torch.long)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    \n",
    "    # Cargar datos de embeddings desde el archivo CSV\n",
    "    embeddings_df = pd.read_csv('csvs/average_clustering_rs.csv', header=None) #modificar csv a necessitat\n",
    "    x = torch.tensor(embeddings_df.values, dtype=torch.float)\n",
    "\n",
    "    edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(), edge_weight=edge_weight, y=y)\n",
    "    #print(x)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7b23046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270, 76, 76)\n",
      "(270,)\n"
     ]
    }
   ],
   "source": [
    "data = data_RS_combined\n",
    "print(data.shape)\n",
    "\n",
    "target = demographics_N['mstype'].to_numpy()\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b874465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.7013\n",
      "Train loss at epoch 2: 0.6966\n",
      "Train loss at epoch 3: 0.6940\n",
      "Train loss at epoch 4: 0.6927\n",
      "Train loss at epoch 5: 0.6905\n",
      "Train loss at epoch 6: 0.6898\n",
      "Train loss at epoch 7: 0.6897\n",
      "Train loss at epoch 8: 0.6893\n",
      "Train loss at epoch 9: 0.6893\n",
      "Train loss at epoch 10: 0.6892\n",
      "Train loss at epoch 11: 0.6895\n",
      "Train loss at epoch 12: 0.6892\n",
      "Train loss at epoch 13: 0.6893\n",
      "Train loss at epoch 14: 0.6894\n",
      "Train loss at epoch 15: 0.6894\n",
      "Train loss at epoch 16: 0.6893\n",
      "Train loss at epoch 17: 0.6894\n",
      "Train loss at epoch 18: 0.6893\n",
      "Train loss at epoch 19: 0.6893\n",
      "Train loss at epoch 20: 0.6893\n",
      "Test AUC: 0.50\n",
      "Fold: 2\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.6901\n",
      "Train loss at epoch 2: 0.6896\n",
      "Train loss at epoch 3: 0.6896\n",
      "Train loss at epoch 4: 0.6893\n",
      "Train loss at epoch 5: 0.6893\n",
      "Train loss at epoch 6: 0.6894\n",
      "Train loss at epoch 7: 0.6892\n",
      "Train loss at epoch 8: 0.6894\n",
      "Train loss at epoch 9: 0.6892\n",
      "Train loss at epoch 10: 0.6893\n",
      "Train loss at epoch 11: 0.6893\n",
      "Train loss at epoch 12: 0.6898\n",
      "Train loss at epoch 13: 0.6893\n",
      "Train loss at epoch 14: 0.6893\n",
      "Train loss at epoch 15: 0.6895\n",
      "Train loss at epoch 16: 0.6896\n",
      "Train loss at epoch 17: 0.6894\n",
      "Train loss at epoch 18: 0.6894\n",
      "Train loss at epoch 19: 0.6892\n",
      "Train loss at epoch 20: 0.6900\n",
      "Test AUC: 0.50\n",
      "Fold: 3\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.6912\n",
      "Train loss at epoch 2: 0.6904\n",
      "Train loss at epoch 3: 0.6899\n",
      "Train loss at epoch 4: 0.6897\n",
      "Train loss at epoch 5: 0.6894\n",
      "Train loss at epoch 6: 0.6894\n",
      "Train loss at epoch 7: 0.6894\n",
      "Train loss at epoch 8: 0.6892\n",
      "Train loss at epoch 9: 0.6892\n",
      "Train loss at epoch 10: 0.6892\n",
      "Train loss at epoch 11: 0.6892\n",
      "Train loss at epoch 12: 0.6892\n",
      "Train loss at epoch 13: 0.6893\n",
      "Train loss at epoch 14: 0.6895\n",
      "Train loss at epoch 15: 0.6894\n",
      "Train loss at epoch 16: 0.6893\n",
      "Train loss at epoch 17: 0.6893\n",
      "Train loss at epoch 18: 0.6894\n",
      "Train loss at epoch 19: 0.6892\n",
      "Train loss at epoch 20: 0.6893\n",
      "Test AUC: 0.50\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "preds = np.zeros(data.shape[0])\n",
    "fold = 0\n",
    "\n",
    "for train_index, test_index in skf.split(data, target):\n",
    "    fold += 1\n",
    "    print(\"Fold: {}\".format(fold))\n",
    "\n",
    "    # split dataset\n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    prop_train = np.where(y_train == 1)[0].shape[0] / y_train.shape[0]\n",
    "    prop_test = np.where(y_test == 1)[0].shape[0] / y_test.shape[0]\n",
    "    print(\"Train set size     : {}\".format(X_train.shape))\n",
    "    print(\"Test set size      : {}\".format(X_test.shape))\n",
    "    print(\"Train set % of pwMS: {:.4f} ({})\".format(prop_train, y_train.sum()))\n",
    "    print(\"Test set % of pwMS : {:.4f} ({})\".format(prop_test, y_test.sum()))\n",
    "\n",
    "    # list of Data structures (one for each subject)\n",
    "    train_graphs = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        g = array_to_graph(X_train[i], y_train[i])\n",
    "        train_graphs.append(g)\n",
    "        \n",
    "    test_graphs = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        g = array_to_graph(X_test[i], y_test[i])\n",
    "        test_graphs.append(g)\n",
    "\n",
    "    # create the model\n",
    "    model = GCN()\n",
    "    model = model.to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # train function\n",
    "    def train():\n",
    "        model.train()\n",
    "        \n",
    "        train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n",
    "\n",
    "        loss_all = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            label = batch.y\n",
    "            label = F.one_hot(label, num_classes=2)\n",
    "            label = label.type(torch.FloatTensor)\n",
    "            label = label.to(device)\n",
    "            loss = loss_fn(output, label)\n",
    "            loss.backward()\n",
    "            loss_all += batch.num_graphs * loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        return loss_all / len(train_graphs)\n",
    "\n",
    "    # train for N epochs\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        loss_value = train()\n",
    "        print(\"Train loss at epoch {}: {:.4f}\".format(epoch + 1, loss_value))\n",
    "\n",
    "    # test phase \n",
    "    test_loader = DataLoader(test_graphs, batch_size=len(test_graphs), shuffle=False)\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        test_preds = F.softmax(model(batch), dim=1).detach().numpy()\n",
    "    \n",
    "    test_preds = test_preds[:, 1]\n",
    "    preds[test_index] = test_preds\n",
    "    \n",
    "    auc_roc = roc_auc_score(y_test, test_preds)\n",
    "    print(\"Test AUC: {:.2f}\".format(auc_roc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de087375",
   "metadata": {},
   "source": [
    "## WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23bca7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# CAL DEFINIR ELS EMBEDDINGS ES FARAN SERVIR\n",
    "# EN AQUEST CODI, NOMÉS ÉS UN VECTOR DE (node_dim X 1's)\n",
    "node_dim = 50  # Asegúrate de que esto coincida con las dimensiones del archivo\n",
    "\n",
    "def array_to_graph(data, y, th=0.0):\n",
    "    num_nodes = data.shape[0]\n",
    "    \n",
    "    edge_index = []\n",
    "    edge_weight = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:\n",
    "                if data[i, j] >= th:\n",
    "                    edge_index.append([i, j])\n",
    "                    edge_weight.append(data[i, j])\n",
    "        \n",
    "    y = torch.tensor([int(y)], dtype=torch.long)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    \n",
    "    # Cargar datos de embeddings desde el archivo CSV\n",
    "    embeddings_df = pd.read_csv('csvs/embeddings_FA.csv', header=None) #modificar csv a necessitat\n",
    "    x = torch.tensor(embeddings_df.values, dtype=torch.float)\n",
    "\n",
    "    edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(), edge_weight=edge_weight, y=y)\n",
    "    #print(x)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b3b65a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270, 76, 76)\n",
      "(270,)\n"
     ]
    }
   ],
   "source": [
    "data = data_FA_combined\n",
    "print(data.shape)\n",
    "\n",
    "target = demographics_N['mstype'].to_numpy()\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "961c9cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 1.1090\n",
      "Train loss at epoch 2: 0.7309\n",
      "Train loss at epoch 3: 0.6965\n",
      "Train loss at epoch 4: 0.7150\n",
      "Train loss at epoch 5: 0.6997\n",
      "Train loss at epoch 6: 0.6808\n",
      "Train loss at epoch 7: 0.6768\n",
      "Train loss at epoch 8: 0.6726\n",
      "Train loss at epoch 9: 0.6686\n",
      "Train loss at epoch 10: 0.6642\n",
      "Train loss at epoch 11: 0.6608\n",
      "Train loss at epoch 12: 0.6579\n",
      "Train loss at epoch 13: 0.6531\n",
      "Train loss at epoch 14: 0.6508\n",
      "Train loss at epoch 15: 0.6451\n",
      "Train loss at epoch 16: 0.6417\n",
      "Train loss at epoch 17: 0.6378\n",
      "Train loss at epoch 18: 0.6324\n",
      "Train loss at epoch 19: 0.6286\n",
      "Train loss at epoch 20: 0.6242\n",
      "Test AUC: 0.80\n",
      "Fold: 2\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.7135\n",
      "Train loss at epoch 2: 0.6962\n",
      "Train loss at epoch 3: 0.6807\n",
      "Train loss at epoch 4: 0.6809\n",
      "Train loss at epoch 5: 0.6737\n",
      "Train loss at epoch 6: 0.6723\n",
      "Train loss at epoch 7: 0.6719\n",
      "Train loss at epoch 8: 0.6615\n",
      "Train loss at epoch 9: 0.6624\n",
      "Train loss at epoch 10: 0.6537\n",
      "Train loss at epoch 11: 0.6519\n",
      "Train loss at epoch 12: 0.6462\n",
      "Train loss at epoch 13: 0.6459\n",
      "Train loss at epoch 14: 0.6484\n",
      "Train loss at epoch 15: 0.6357\n",
      "Train loss at epoch 16: 0.6316\n",
      "Train loss at epoch 17: 0.6301\n",
      "Train loss at epoch 18: 0.6248\n",
      "Train loss at epoch 19: 0.6180\n",
      "Train loss at epoch 20: 0.6213\n",
      "Test AUC: 0.97\n",
      "Fold: 3\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.6886\n",
      "Train loss at epoch 2: 0.6776\n",
      "Train loss at epoch 3: 0.6790\n",
      "Train loss at epoch 4: 0.6716\n",
      "Train loss at epoch 5: 0.6682\n",
      "Train loss at epoch 6: 0.6665\n",
      "Train loss at epoch 7: 0.6622\n",
      "Train loss at epoch 8: 0.6619\n",
      "Train loss at epoch 9: 0.6553\n",
      "Train loss at epoch 10: 0.6524\n",
      "Train loss at epoch 11: 0.6479\n",
      "Train loss at epoch 12: 0.6456\n",
      "Train loss at epoch 13: 0.6401\n",
      "Train loss at epoch 14: 0.6364\n",
      "Train loss at epoch 15: 0.6400\n",
      "Train loss at epoch 16: 0.6293\n",
      "Train loss at epoch 17: 0.6307\n",
      "Train loss at epoch 18: 0.6219\n",
      "Train loss at epoch 19: 0.6198\n",
      "Train loss at epoch 20: 0.6187\n",
      "Test AUC: 1.00\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "preds = np.zeros(data.shape[0])\n",
    "fold = 0\n",
    "\n",
    "for train_index, test_index in skf.split(data, target):\n",
    "    fold += 1\n",
    "    print(\"Fold: {}\".format(fold))\n",
    "\n",
    "    # split dataset\n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    prop_train = np.where(y_train == 1)[0].shape[0] / y_train.shape[0]\n",
    "    prop_test = np.where(y_test == 1)[0].shape[0] / y_test.shape[0]\n",
    "    print(\"Train set size     : {}\".format(X_train.shape))\n",
    "    print(\"Test set size      : {}\".format(X_test.shape))\n",
    "    print(\"Train set % of pwMS: {:.4f} ({})\".format(prop_train, y_train.sum()))\n",
    "    print(\"Test set % of pwMS : {:.4f} ({})\".format(prop_test, y_test.sum()))\n",
    "\n",
    "    # list of Data structures (one for each subject)\n",
    "    train_graphs = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        g = array_to_graph(X_train[i], y_train[i])\n",
    "        train_graphs.append(g)\n",
    "        \n",
    "    test_graphs = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        g = array_to_graph(X_test[i], y_test[i])\n",
    "        test_graphs.append(g)\n",
    "\n",
    "    # create the model\n",
    "    model = GCN()\n",
    "    model = model.to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # train function\n",
    "    def train():\n",
    "        model.train()\n",
    "        \n",
    "        train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n",
    "\n",
    "        loss_all = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            label = batch.y\n",
    "            label = F.one_hot(label, num_classes=2)\n",
    "            label = label.type(torch.FloatTensor)\n",
    "            label = label.to(device)\n",
    "            loss = loss_fn(output, label)\n",
    "            loss.backward()\n",
    "            loss_all += batch.num_graphs * loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        return loss_all / len(train_graphs)\n",
    "\n",
    "    # train for N epochs\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        loss_value = train()\n",
    "        print(\"Train loss at epoch {}: {:.4f}\".format(epoch + 1, loss_value))\n",
    "\n",
    "    # test phase \n",
    "    test_loader = DataLoader(test_graphs, batch_size=len(test_graphs), shuffle=False)\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        test_preds = F.softmax(model(batch), dim=1).detach().numpy()\n",
    "    \n",
    "    test_preds = test_preds[:, 1]\n",
    "    preds[test_index] = test_preds\n",
    "    \n",
    "    auc_roc = roc_auc_score(y_test, test_preds)\n",
    "    print(\"Test AUC: {:.2f}\".format(auc_roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b5644a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# CAL DEFINIR ELS EMBEDDINGS ES FARAN SERVIR\n",
    "# EN AQUEST CODI, NOMÉS ÉS UN VECTOR DE (node_dim X 1's)\n",
    "node_dim = 50  # Asegúrate de que esto coincida con las dimensiones del archivo\n",
    "\n",
    "def array_to_graph(data, y, th=0.0):\n",
    "    num_nodes = data.shape[0]\n",
    "    \n",
    "    edge_index = []\n",
    "    edge_weight = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:\n",
    "                if data[i, j] >= th:\n",
    "                    edge_index.append([i, j])\n",
    "                    edge_weight.append(data[i, j])\n",
    "        \n",
    "    y = torch.tensor([int(y)], dtype=torch.long)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    \n",
    "    # Cargar datos de embeddings desde el archivo CSV\n",
    "    embeddings_df = pd.read_csv('csvs/embeddings_GM.csv', header=None) #modificar csv a necessitat\n",
    "    x = torch.tensor(embeddings_df.values, dtype=torch.float)\n",
    "\n",
    "    edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(), edge_weight=edge_weight, y=y)\n",
    "    #print(x)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7308d171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270, 76, 76)\n",
      "(270,)\n"
     ]
    }
   ],
   "source": [
    "data = data_GM_combined\n",
    "print(data.shape)\n",
    "\n",
    "target = demographics_N['mstype'].to_numpy()\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dc02df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 1.0291\n",
      "Train loss at epoch 2: 0.5033\n",
      "Train loss at epoch 3: 0.5709\n",
      "Train loss at epoch 4: 0.5403\n",
      "Train loss at epoch 5: 0.4620\n",
      "Train loss at epoch 6: 0.3844\n",
      "Train loss at epoch 7: 0.3457\n",
      "Train loss at epoch 8: 0.3288\n",
      "Train loss at epoch 9: 0.3000\n",
      "Train loss at epoch 10: 0.2749\n",
      "Train loss at epoch 11: 0.2572\n",
      "Train loss at epoch 12: 0.2387\n",
      "Train loss at epoch 13: 0.2221\n",
      "Train loss at epoch 14: 0.2055\n",
      "Train loss at epoch 15: 0.1959\n",
      "Train loss at epoch 16: 0.1808\n",
      "Train loss at epoch 17: 0.1713\n",
      "Train loss at epoch 18: 0.1695\n",
      "Train loss at epoch 19: 0.1584\n",
      "Train loss at epoch 20: 0.1545\n",
      "Test AUC: 0.76\n",
      "Fold: 2\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.5933\n",
      "Train loss at epoch 2: 0.5927\n",
      "Train loss at epoch 3: 0.5583\n",
      "Train loss at epoch 4: 0.5430\n",
      "Train loss at epoch 5: 0.5217\n",
      "Train loss at epoch 6: 0.5074\n",
      "Train loss at epoch 7: 0.4976\n",
      "Train loss at epoch 8: 0.5193\n",
      "Train loss at epoch 9: 0.4822\n",
      "Train loss at epoch 10: 0.4513\n",
      "Train loss at epoch 11: 0.4410\n",
      "Train loss at epoch 12: 0.4172\n",
      "Train loss at epoch 13: 0.4083\n",
      "Train loss at epoch 14: 0.3996\n",
      "Train loss at epoch 15: 0.3984\n",
      "Train loss at epoch 16: 0.3910\n",
      "Train loss at epoch 17: 0.3796\n",
      "Train loss at epoch 18: 0.3832\n",
      "Train loss at epoch 19: 0.3792\n",
      "Train loss at epoch 20: 0.3742\n",
      "Test AUC: 1.00\n",
      "Fold: 3\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 1.0427\n",
      "Train loss at epoch 2: 0.7982\n",
      "Train loss at epoch 3: 0.7712\n",
      "Train loss at epoch 4: 0.5743\n",
      "Train loss at epoch 5: 0.5532\n",
      "Train loss at epoch 6: 0.4958\n",
      "Train loss at epoch 7: 0.4929\n",
      "Train loss at epoch 8: 0.4754\n",
      "Train loss at epoch 9: 0.4670\n",
      "Train loss at epoch 10: 0.4557\n",
      "Train loss at epoch 11: 0.4384\n",
      "Train loss at epoch 12: 0.4299\n",
      "Train loss at epoch 13: 0.4127\n",
      "Train loss at epoch 14: 0.4033\n",
      "Train loss at epoch 15: 0.3950\n",
      "Train loss at epoch 16: 0.3827\n",
      "Train loss at epoch 17: 0.3768\n",
      "Train loss at epoch 18: 0.3624\n",
      "Train loss at epoch 19: 0.3683\n",
      "Train loss at epoch 20: 0.3526\n",
      "Test AUC: 0.98\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "preds = np.zeros(data.shape[0])\n",
    "fold = 0\n",
    "\n",
    "for train_index, test_index in skf.split(data, target):\n",
    "    fold += 1\n",
    "    print(\"Fold: {}\".format(fold))\n",
    "\n",
    "    # split dataset\n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    prop_train = np.where(y_train == 1)[0].shape[0] / y_train.shape[0]\n",
    "    prop_test = np.where(y_test == 1)[0].shape[0] / y_test.shape[0]\n",
    "    print(\"Train set size     : {}\".format(X_train.shape))\n",
    "    print(\"Test set size      : {}\".format(X_test.shape))\n",
    "    print(\"Train set % of pwMS: {:.4f} ({})\".format(prop_train, y_train.sum()))\n",
    "    print(\"Test set % of pwMS : {:.4f} ({})\".format(prop_test, y_test.sum()))\n",
    "\n",
    "    # list of Data structures (one for each subject)\n",
    "    train_graphs = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        g = array_to_graph(X_train[i], y_train[i])\n",
    "        train_graphs.append(g)\n",
    "        \n",
    "    test_graphs = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        g = array_to_graph(X_test[i], y_test[i])\n",
    "        test_graphs.append(g)\n",
    "\n",
    "    # create the model\n",
    "    model = GCN()\n",
    "    model = model.to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # train function\n",
    "    def train():\n",
    "        model.train()\n",
    "        \n",
    "        train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n",
    "\n",
    "        loss_all = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            label = batch.y\n",
    "            label = F.one_hot(label, num_classes=2)\n",
    "            label = label.type(torch.FloatTensor)\n",
    "            label = label.to(device)\n",
    "            loss = loss_fn(output, label)\n",
    "            loss.backward()\n",
    "            loss_all += batch.num_graphs * loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        return loss_all / len(train_graphs)\n",
    "\n",
    "    # train for N epochs\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        loss_value = train()\n",
    "        print(\"Train loss at epoch {}: {:.4f}\".format(epoch + 1, loss_value))\n",
    "\n",
    "    # test phase \n",
    "    test_loader = DataLoader(test_graphs, batch_size=len(test_graphs), shuffle=False)\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        test_preds = F.softmax(model(batch), dim=1).detach().numpy()\n",
    "    \n",
    "    test_preds = test_preds[:, 1]\n",
    "    preds[test_index] = test_preds\n",
    "    \n",
    "    auc_roc = roc_auc_score(y_test, test_preds)\n",
    "    print(\"Test AUC: {:.2f}\".format(auc_roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbb441ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# CAL DEFINIR ELS EMBEDDINGS ES FARAN SERVIR\n",
    "# EN AQUEST CODI, NOMÉS ÉS UN VECTOR DE (node_dim X 1's)\n",
    "node_dim = 50  # Asegúrate de que esto coincida con las dimensiones del archivo\n",
    "\n",
    "def array_to_graph(data, y, th=0.0):\n",
    "    num_nodes = data.shape[0]\n",
    "    \n",
    "    edge_index = []\n",
    "    edge_weight = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:\n",
    "                if data[i, j] >= th:\n",
    "                    edge_index.append([i, j])\n",
    "                    edge_weight.append(data[i, j])\n",
    "        \n",
    "    y = torch.tensor([int(y)], dtype=torch.long)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    \n",
    "    # Cargar datos de embeddings desde el archivo CSV\n",
    "    embeddings_df = pd.read_csv('csvs/embeddings_RS.csv', header=None) #modificar csv a necessitat\n",
    "    x = torch.tensor(embeddings_df.values, dtype=torch.float)\n",
    "\n",
    "    edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(), edge_weight=edge_weight, y=y)\n",
    "    #print(x)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d0ec053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270, 76, 76)\n",
      "(270,)\n"
     ]
    }
   ],
   "source": [
    "data = data_RS_combined\n",
    "print(data.shape)\n",
    "\n",
    "target = demographics_N['mstype'].to_numpy()\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "706bbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.6847\n",
      "Train loss at epoch 2: 0.6573\n",
      "Train loss at epoch 3: 0.6421\n",
      "Train loss at epoch 4: 0.6318\n",
      "Train loss at epoch 5: 0.6207\n",
      "Train loss at epoch 6: 0.6174\n",
      "Train loss at epoch 7: 0.6070\n",
      "Train loss at epoch 8: 0.5893\n",
      "Train loss at epoch 9: 0.5909\n",
      "Train loss at epoch 10: 0.5771\n",
      "Train loss at epoch 11: 0.5722\n",
      "Train loss at epoch 12: 0.5623\n",
      "Train loss at epoch 13: 0.5598\n",
      "Train loss at epoch 14: 0.5566\n",
      "Train loss at epoch 15: 0.5478\n",
      "Train loss at epoch 16: 0.5481\n",
      "Train loss at epoch 17: 0.5338\n",
      "Train loss at epoch 18: 0.5332\n",
      "Train loss at epoch 19: 0.5249\n",
      "Train loss at epoch 20: 0.5270\n",
      "Test AUC: 0.63\n",
      "Fold: 2\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.6934\n",
      "Train loss at epoch 2: 0.6810\n",
      "Train loss at epoch 3: 0.6702\n",
      "Train loss at epoch 4: 0.6633\n",
      "Train loss at epoch 5: 0.6604\n",
      "Train loss at epoch 6: 0.6471\n",
      "Train loss at epoch 7: 0.6413\n",
      "Train loss at epoch 8: 0.6407\n",
      "Train loss at epoch 9: 0.6317\n",
      "Train loss at epoch 10: 0.6251\n",
      "Train loss at epoch 11: 0.6175\n",
      "Train loss at epoch 12: 0.6150\n",
      "Train loss at epoch 13: 0.6094\n",
      "Train loss at epoch 14: 0.6101\n",
      "Train loss at epoch 15: 0.6010\n",
      "Train loss at epoch 16: 0.5959\n",
      "Train loss at epoch 17: 0.5935\n",
      "Train loss at epoch 18: 0.5916\n",
      "Train loss at epoch 19: 0.5877\n",
      "Train loss at epoch 20: 0.5856\n",
      "Test AUC: 0.81\n",
      "Fold: 3\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.6976\n",
      "Train loss at epoch 2: 0.6890\n",
      "Train loss at epoch 3: 0.6769\n",
      "Train loss at epoch 4: 0.6785\n",
      "Train loss at epoch 5: 0.6670\n",
      "Train loss at epoch 6: 0.6616\n",
      "Train loss at epoch 7: 0.6605\n",
      "Train loss at epoch 8: 0.6505\n",
      "Train loss at epoch 9: 0.6470\n",
      "Train loss at epoch 10: 0.6436\n",
      "Train loss at epoch 11: 0.6397\n",
      "Train loss at epoch 12: 0.6399\n",
      "Train loss at epoch 13: 0.6329\n",
      "Train loss at epoch 14: 0.6287\n",
      "Train loss at epoch 15: 0.6237\n",
      "Train loss at epoch 16: 0.6239\n",
      "Train loss at epoch 17: 0.6196\n",
      "Train loss at epoch 18: 0.6168\n",
      "Train loss at epoch 19: 0.6158\n",
      "Train loss at epoch 20: 0.6132\n",
      "Test AUC: 0.89\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "preds = np.zeros(data.shape[0])\n",
    "fold = 0\n",
    "\n",
    "for train_index, test_index in skf.split(data, target):\n",
    "    fold += 1\n",
    "    print(\"Fold: {}\".format(fold))\n",
    "\n",
    "    # split dataset\n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    prop_train = np.where(y_train == 1)[0].shape[0] / y_train.shape[0]\n",
    "    prop_test = np.where(y_test == 1)[0].shape[0] / y_test.shape[0]\n",
    "    print(\"Train set size     : {}\".format(X_train.shape))\n",
    "    print(\"Test set size      : {}\".format(X_test.shape))\n",
    "    print(\"Train set % of pwMS: {:.4f} ({})\".format(prop_train, y_train.sum()))\n",
    "    print(\"Test set % of pwMS : {:.4f} ({})\".format(prop_test, y_test.sum()))\n",
    "\n",
    "    # list of Data structures (one for each subject)\n",
    "    train_graphs = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        g = array_to_graph(X_train[i], y_train[i])\n",
    "        train_graphs.append(g)\n",
    "        \n",
    "    test_graphs = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        g = array_to_graph(X_test[i], y_test[i])\n",
    "        test_graphs.append(g)\n",
    "\n",
    "    # create the model\n",
    "    model = GCN()\n",
    "    model = model.to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # train function\n",
    "    def train():\n",
    "        model.train()\n",
    "        \n",
    "        train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n",
    "\n",
    "        loss_all = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            label = batch.y\n",
    "            label = F.one_hot(label, num_classes=2)\n",
    "            label = label.type(torch.FloatTensor)\n",
    "            label = label.to(device)\n",
    "            loss = loss_fn(output, label)\n",
    "            loss.backward()\n",
    "            loss_all += batch.num_graphs * loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        return loss_all / len(train_graphs)\n",
    "\n",
    "    # train for N epochs\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        loss_value = train()\n",
    "        print(\"Train loss at epoch {}: {:.4f}\".format(epoch + 1, loss_value))\n",
    "\n",
    "    # test phase \n",
    "    test_loader = DataLoader(test_graphs, batch_size=len(test_graphs), shuffle=False)\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        test_preds = F.softmax(model(batch), dim=1).detach().numpy()\n",
    "    \n",
    "    test_preds = test_preds[:, 1]\n",
    "    preds[test_index] = test_preds\n",
    "    \n",
    "    auc_roc = roc_auc_score(y_test, test_preds)\n",
    "    print(\"Test AUC: {:.2f}\".format(auc_roc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6104453",
   "metadata": {},
   "source": [
    "## NODE2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "baf71796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# CAL DEFINIR ELS EMBEDDINGS ES FARAN SERVIR\n",
    "# EN AQUEST CODI, NOMÉS ÉS UN VECTOR DE (node_dim X 1's)\n",
    "node_dim = 25  # Asegúrate de que esto coincida con las dimensiones del archivo\n",
    "\n",
    "def array_to_graph(data, y, th=0.0):\n",
    "    num_nodes = data.shape[0]\n",
    "    \n",
    "    edge_index = []\n",
    "    edge_weight = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:\n",
    "                if data[i, j] >= th:\n",
    "                    edge_index.append([i, j])\n",
    "                    edge_weight.append(data[i, j])\n",
    "        \n",
    "    y = torch.tensor([int(y)], dtype=torch.long)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    \n",
    "    # Cargar datos de embeddings desde el archivo CSV\n",
    "    embeddings_df = pd.read_csv('csvs/embeddings_FA_n2v.csv', header=None) #modificar csv a necessitat\n",
    "    x = torch.tensor(embeddings_df.values, dtype=torch.float)\n",
    "\n",
    "    edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(), edge_weight=edge_weight, y=y)\n",
    "    #print(x)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f074c36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270, 76, 76)\n",
      "(270,)\n"
     ]
    }
   ],
   "source": [
    "data = data_FA_combined\n",
    "print(data.shape)\n",
    "\n",
    "target = demographics_N['mstype'].to_numpy()\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2556d825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.6784\n",
      "Train loss at epoch 2: 0.6698\n",
      "Train loss at epoch 3: 0.6668\n",
      "Train loss at epoch 4: 0.6593\n",
      "Train loss at epoch 5: 0.6532\n",
      "Train loss at epoch 6: 0.6501\n",
      "Train loss at epoch 7: 0.6434\n",
      "Train loss at epoch 8: 0.6380\n",
      "Train loss at epoch 9: 0.6315\n",
      "Train loss at epoch 10: 0.6258\n",
      "Train loss at epoch 11: 0.6213\n",
      "Train loss at epoch 12: 0.6138\n",
      "Train loss at epoch 13: 0.6068\n",
      "Train loss at epoch 14: 0.6002\n",
      "Train loss at epoch 15: 0.5948\n",
      "Train loss at epoch 16: 0.5858\n",
      "Train loss at epoch 17: 0.5799\n",
      "Train loss at epoch 18: 0.5721\n",
      "Train loss at epoch 19: 0.5638\n",
      "Train loss at epoch 20: 0.5584\n",
      "Test AUC: 0.79\n",
      "Fold: 2\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.6924\n",
      "Train loss at epoch 2: 0.6791\n",
      "Train loss at epoch 3: 0.6801\n",
      "Train loss at epoch 4: 0.6732\n",
      "Train loss at epoch 5: 0.6676\n",
      "Train loss at epoch 6: 0.6662\n",
      "Train loss at epoch 7: 0.6628\n",
      "Train loss at epoch 8: 0.6589\n",
      "Train loss at epoch 9: 0.6543\n",
      "Train loss at epoch 10: 0.6512\n",
      "Train loss at epoch 11: 0.6465\n",
      "Train loss at epoch 12: 0.6422\n",
      "Train loss at epoch 13: 0.6389\n",
      "Train loss at epoch 14: 0.6334\n",
      "Train loss at epoch 15: 0.6298\n",
      "Train loss at epoch 16: 0.6242\n",
      "Train loss at epoch 17: 0.6226\n",
      "Train loss at epoch 18: 0.6173\n",
      "Train loss at epoch 19: 0.6111\n",
      "Train loss at epoch 20: 0.6056\n",
      "Test AUC: 0.96\n",
      "Fold: 3\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.6743\n",
      "Train loss at epoch 2: 0.6675\n",
      "Train loss at epoch 3: 0.6644\n",
      "Train loss at epoch 4: 0.6622\n",
      "Train loss at epoch 5: 0.6590\n",
      "Train loss at epoch 6: 0.6565\n",
      "Train loss at epoch 7: 0.6540\n",
      "Train loss at epoch 8: 0.6526\n",
      "Train loss at epoch 9: 0.6490\n",
      "Train loss at epoch 10: 0.6458\n",
      "Train loss at epoch 11: 0.6446\n",
      "Train loss at epoch 12: 0.6406\n",
      "Train loss at epoch 13: 0.6374\n",
      "Train loss at epoch 14: 0.6340\n",
      "Train loss at epoch 15: 0.6307\n",
      "Train loss at epoch 16: 0.6268\n",
      "Train loss at epoch 17: 0.6252\n",
      "Train loss at epoch 18: 0.6197\n",
      "Train loss at epoch 19: 0.6182\n",
      "Train loss at epoch 20: 0.6145\n",
      "Test AUC: 1.00\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "preds = np.zeros(data.shape[0])\n",
    "fold = 0\n",
    "\n",
    "for train_index, test_index in skf.split(data, target):\n",
    "    fold += 1\n",
    "    print(\"Fold: {}\".format(fold))\n",
    "\n",
    "    # split dataset\n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    prop_train = np.where(y_train == 1)[0].shape[0] / y_train.shape[0]\n",
    "    prop_test = np.where(y_test == 1)[0].shape[0] / y_test.shape[0]\n",
    "    print(\"Train set size     : {}\".format(X_train.shape))\n",
    "    print(\"Test set size      : {}\".format(X_test.shape))\n",
    "    print(\"Train set % of pwMS: {:.4f} ({})\".format(prop_train, y_train.sum()))\n",
    "    print(\"Test set % of pwMS : {:.4f} ({})\".format(prop_test, y_test.sum()))\n",
    "\n",
    "    # list of Data structures (one for each subject)\n",
    "    train_graphs = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        g = array_to_graph(X_train[i], y_train[i])\n",
    "        train_graphs.append(g)\n",
    "        \n",
    "    test_graphs = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        g = array_to_graph(X_test[i], y_test[i])\n",
    "        test_graphs.append(g)\n",
    "\n",
    "    # create the model\n",
    "    model = GCN()\n",
    "    model = model.to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # train function\n",
    "    def train():\n",
    "        model.train()\n",
    "        \n",
    "        train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n",
    "\n",
    "        loss_all = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            label = batch.y\n",
    "            label = F.one_hot(label, num_classes=2)\n",
    "            label = label.type(torch.FloatTensor)\n",
    "            label = label.to(device)\n",
    "            loss = loss_fn(output, label)\n",
    "            loss.backward()\n",
    "            loss_all += batch.num_graphs * loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        return loss_all / len(train_graphs)\n",
    "\n",
    "    # train for N epochs\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        loss_value = train()\n",
    "        print(\"Train loss at epoch {}: {:.4f}\".format(epoch + 1, loss_value))\n",
    "\n",
    "    # test phase \n",
    "    test_loader = DataLoader(test_graphs, batch_size=len(test_graphs), shuffle=False)\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        test_preds = F.softmax(model(batch), dim=1).detach().numpy()\n",
    "    \n",
    "    test_preds = test_preds[:, 1]\n",
    "    preds[test_index] = test_preds\n",
    "    \n",
    "    auc_roc = roc_auc_score(y_test, test_preds)\n",
    "    print(\"Test AUC: {:.2f}\".format(auc_roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9439458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# CAL DEFINIR ELS EMBEDDINGS ES FARAN SERVIR\n",
    "# EN AQUEST CODI, NOMÉS ÉS UN VECTOR DE (node_dim X 1's)\n",
    "node_dim = 25  # Asegúrate de que esto coincida con las dimensiones del archivo\n",
    "\n",
    "def array_to_graph(data, y, th=0.0):\n",
    "    num_nodes = data.shape[0]\n",
    "    \n",
    "    edge_index = []\n",
    "    edge_weight = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:\n",
    "                if data[i, j] >= th:\n",
    "                    edge_index.append([i, j])\n",
    "                    edge_weight.append(data[i, j])\n",
    "        \n",
    "    y = torch.tensor([int(y)], dtype=torch.long)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    \n",
    "    # Cargar datos de embeddings desde el archivo CSV\n",
    "    embeddings_df = pd.read_csv('csvs/embeddings_GM_n2v.csv', header=None) #modificar csv a necessitat\n",
    "    x = torch.tensor(embeddings_df.values, dtype=torch.float)\n",
    "\n",
    "    edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(), edge_weight=edge_weight, y=y)\n",
    "    #print(x)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51a1d6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270, 76, 76)\n",
      "(270,)\n"
     ]
    }
   ],
   "source": [
    "data = data_GM_combined\n",
    "print(data.shape)\n",
    "\n",
    "target = demographics_N['mstype'].to_numpy()\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "359d18c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.4104\n",
      "Train loss at epoch 2: 0.3723\n",
      "Train loss at epoch 3: 0.3483\n",
      "Train loss at epoch 4: 0.3268\n",
      "Train loss at epoch 5: 0.3027\n",
      "Train loss at epoch 6: 0.2781\n",
      "Train loss at epoch 7: 0.2611\n",
      "Train loss at epoch 8: 0.2370\n",
      "Train loss at epoch 9: 0.2191\n",
      "Train loss at epoch 10: 0.2066\n",
      "Train loss at epoch 11: 0.1918\n",
      "Train loss at epoch 12: 0.1807\n",
      "Train loss at epoch 13: 0.1725\n",
      "Train loss at epoch 14: 0.1630\n",
      "Train loss at epoch 15: 0.1560\n",
      "Train loss at epoch 16: 0.1507\n",
      "Train loss at epoch 17: 0.1502\n",
      "Train loss at epoch 18: 0.1456\n",
      "Train loss at epoch 19: 0.1419\n",
      "Train loss at epoch 20: 0.1386\n",
      "Test AUC: 0.76\n",
      "Fold: 2\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 1.0161\n",
      "Train loss at epoch 2: 0.6791\n",
      "Train loss at epoch 3: 0.6911\n",
      "Train loss at epoch 4: 0.6221\n",
      "Train loss at epoch 5: 0.5685\n",
      "Train loss at epoch 6: 0.5792\n",
      "Train loss at epoch 7: 0.5500\n",
      "Train loss at epoch 8: 0.5389\n",
      "Train loss at epoch 9: 0.5224\n",
      "Train loss at epoch 10: 0.5104\n",
      "Train loss at epoch 11: 0.4990\n",
      "Train loss at epoch 12: 0.4849\n",
      "Train loss at epoch 13: 0.4792\n",
      "Train loss at epoch 14: 0.4643\n",
      "Train loss at epoch 15: 0.4539\n",
      "Train loss at epoch 16: 0.4424\n",
      "Train loss at epoch 17: 0.4352\n",
      "Train loss at epoch 18: 0.4267\n",
      "Train loss at epoch 19: 0.4194\n",
      "Train loss at epoch 20: 0.4130\n",
      "Test AUC: 1.00\n",
      "Fold: 3\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.6094\n",
      "Train loss at epoch 2: 0.5314\n",
      "Train loss at epoch 3: 0.5255\n",
      "Train loss at epoch 4: 0.4921\n",
      "Train loss at epoch 5: 0.4891\n",
      "Train loss at epoch 6: 0.4720\n",
      "Train loss at epoch 7: 0.4598\n",
      "Train loss at epoch 8: 0.4449\n",
      "Train loss at epoch 9: 0.4347\n",
      "Train loss at epoch 10: 0.4229\n",
      "Train loss at epoch 11: 0.4127\n",
      "Train loss at epoch 12: 0.4015\n",
      "Train loss at epoch 13: 0.3932\n",
      "Train loss at epoch 14: 0.3836\n",
      "Train loss at epoch 15: 0.3772\n",
      "Train loss at epoch 16: 0.3763\n",
      "Train loss at epoch 17: 0.3632\n",
      "Train loss at epoch 18: 0.3581\n",
      "Train loss at epoch 19: 0.3521\n",
      "Train loss at epoch 20: 0.3473\n",
      "Test AUC: 0.98\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "preds = np.zeros(data.shape[0])\n",
    "fold = 0\n",
    "\n",
    "for train_index, test_index in skf.split(data, target):\n",
    "    fold += 1\n",
    "    print(\"Fold: {}\".format(fold))\n",
    "\n",
    "    # split dataset\n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    prop_train = np.where(y_train == 1)[0].shape[0] / y_train.shape[0]\n",
    "    prop_test = np.where(y_test == 1)[0].shape[0] / y_test.shape[0]\n",
    "    print(\"Train set size     : {}\".format(X_train.shape))\n",
    "    print(\"Test set size      : {}\".format(X_test.shape))\n",
    "    print(\"Train set % of pwMS: {:.4f} ({})\".format(prop_train, y_train.sum()))\n",
    "    print(\"Test set % of pwMS : {:.4f} ({})\".format(prop_test, y_test.sum()))\n",
    "\n",
    "    # list of Data structures (one for each subject)\n",
    "    train_graphs = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        g = array_to_graph(X_train[i], y_train[i])\n",
    "        train_graphs.append(g)\n",
    "        \n",
    "    test_graphs = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        g = array_to_graph(X_test[i], y_test[i])\n",
    "        test_graphs.append(g)\n",
    "\n",
    "    # create the model\n",
    "    model = GCN()\n",
    "    model = model.to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # train function\n",
    "    def train():\n",
    "        model.train()\n",
    "        \n",
    "        train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n",
    "\n",
    "        loss_all = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            label = batch.y\n",
    "            label = F.one_hot(label, num_classes=2)\n",
    "            label = label.type(torch.FloatTensor)\n",
    "            label = label.to(device)\n",
    "            loss = loss_fn(output, label)\n",
    "            loss.backward()\n",
    "            loss_all += batch.num_graphs * loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        return loss_all / len(train_graphs)\n",
    "\n",
    "    # train for N epochs\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        loss_value = train()\n",
    "        print(\"Train loss at epoch {}: {:.4f}\".format(epoch + 1, loss_value))\n",
    "\n",
    "    # test phase \n",
    "    test_loader = DataLoader(test_graphs, batch_size=len(test_graphs), shuffle=False)\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        test_preds = F.softmax(model(batch), dim=1).detach().numpy()\n",
    "    \n",
    "    test_preds = test_preds[:, 1]\n",
    "    preds[test_index] = test_preds\n",
    "    \n",
    "    auc_roc = roc_auc_score(y_test, test_preds)\n",
    "    print(\"Test AUC: {:.2f}\".format(auc_roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c594465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# CAL DEFINIR ELS EMBEDDINGS ES FARAN SERVIR\n",
    "# EN AQUEST CODI, NOMÉS ÉS UN VECTOR DE (node_dim X 1's)\n",
    "node_dim = 25  # Asegúrate de que esto coincida con las dimensiones del archivo\n",
    "\n",
    "def array_to_graph(data, y, th=0.0):\n",
    "    num_nodes = data.shape[0]\n",
    "    \n",
    "    edge_index = []\n",
    "    edge_weight = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i != j:\n",
    "                if data[i, j] >= th:\n",
    "                    edge_index.append([i, j])\n",
    "                    edge_weight.append(data[i, j])\n",
    "        \n",
    "    y = torch.tensor([int(y)], dtype=torch.long)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    \n",
    "    # Cargar datos de embeddings desde el archivo CSV\n",
    "    embeddings_df = pd.read_csv('csvs/embeddings_RS_n2v.csv', header=None) #modificar csv a necessitat\n",
    "    x = torch.tensor(embeddings_df.values, dtype=torch.float)\n",
    "\n",
    "    edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(), edge_weight=edge_weight, y=y)\n",
    "    #print(x)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27d7f7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270, 76, 76)\n",
      "(270,)\n"
     ]
    }
   ],
   "source": [
    "data = data_RS_combined\n",
    "print(data.shape)\n",
    "\n",
    "target = demographics_N['mstype'].to_numpy()\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f91c138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.7023\n",
      "Train loss at epoch 2: 0.6863\n",
      "Train loss at epoch 3: 0.6755\n",
      "Train loss at epoch 4: 0.6678\n",
      "Train loss at epoch 5: 0.6590\n",
      "Train loss at epoch 6: 0.6498\n",
      "Train loss at epoch 7: 0.6393\n",
      "Train loss at epoch 8: 0.6315\n",
      "Train loss at epoch 9: 0.6237\n",
      "Train loss at epoch 10: 0.6156\n",
      "Train loss at epoch 11: 0.6084\n",
      "Train loss at epoch 12: 0.5990\n",
      "Train loss at epoch 13: 0.5921\n",
      "Train loss at epoch 14: 0.5852\n",
      "Train loss at epoch 15: 0.5777\n",
      "Train loss at epoch 16: 0.5726\n",
      "Train loss at epoch 17: 0.5649\n",
      "Train loss at epoch 18: 0.5603\n",
      "Train loss at epoch 19: 0.5519\n",
      "Train loss at epoch 20: 0.5460\n",
      "Test AUC: 0.63\n",
      "Fold: 2\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.7256\n",
      "Train loss at epoch 2: 0.7065\n",
      "Train loss at epoch 3: 0.6981\n",
      "Train loss at epoch 4: 0.6925\n",
      "Train loss at epoch 5: 0.6843\n",
      "Train loss at epoch 6: 0.6773\n",
      "Train loss at epoch 7: 0.6724\n",
      "Train loss at epoch 8: 0.6665\n",
      "Train loss at epoch 9: 0.6616\n",
      "Train loss at epoch 10: 0.6563\n",
      "Train loss at epoch 11: 0.6501\n",
      "Train loss at epoch 12: 0.6452\n",
      "Train loss at epoch 13: 0.6403\n",
      "Train loss at epoch 14: 0.6362\n",
      "Train loss at epoch 15: 0.6307\n",
      "Train loss at epoch 16: 0.6253\n",
      "Train loss at epoch 17: 0.6207\n",
      "Train loss at epoch 18: 0.6188\n",
      "Train loss at epoch 19: 0.6127\n",
      "Train loss at epoch 20: 0.6081\n",
      "Test AUC: 0.81\n",
      "Fold: 3\n",
      "Train set size     : (180, 76, 76)\n",
      "Test set size      : (90, 76, 76)\n",
      "Train set % of pwMS: 0.5444 (98)\n",
      "Test set % of pwMS : 0.5444 (49)\n",
      "Train loss at epoch 1: 0.7026\n",
      "Train loss at epoch 2: 0.6879\n",
      "Train loss at epoch 3: 0.6839\n",
      "Train loss at epoch 4: 0.6806\n",
      "Train loss at epoch 5: 0.6759\n",
      "Train loss at epoch 6: 0.6697\n",
      "Train loss at epoch 7: 0.6664\n",
      "Train loss at epoch 8: 0.6621\n",
      "Train loss at epoch 9: 0.6571\n",
      "Train loss at epoch 10: 0.6525\n",
      "Train loss at epoch 11: 0.6485\n",
      "Train loss at epoch 12: 0.6444\n",
      "Train loss at epoch 13: 0.6408\n",
      "Train loss at epoch 14: 0.6373\n",
      "Train loss at epoch 15: 0.6342\n",
      "Train loss at epoch 16: 0.6319\n",
      "Train loss at epoch 17: 0.6291\n",
      "Train loss at epoch 18: 0.6266\n",
      "Train loss at epoch 19: 0.6242\n",
      "Train loss at epoch 20: 0.6235\n",
      "Test AUC: 0.88\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "preds = np.zeros(data.shape[0])\n",
    "fold = 0\n",
    "\n",
    "for train_index, test_index in skf.split(data, target):\n",
    "    fold += 1\n",
    "    print(\"Fold: {}\".format(fold))\n",
    "\n",
    "    # split dataset\n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    \n",
    "    prop_train = np.where(y_train == 1)[0].shape[0] / y_train.shape[0]\n",
    "    prop_test = np.where(y_test == 1)[0].shape[0] / y_test.shape[0]\n",
    "    print(\"Train set size     : {}\".format(X_train.shape))\n",
    "    print(\"Test set size      : {}\".format(X_test.shape))\n",
    "    print(\"Train set % of pwMS: {:.4f} ({})\".format(prop_train, y_train.sum()))\n",
    "    print(\"Test set % of pwMS : {:.4f} ({})\".format(prop_test, y_test.sum()))\n",
    "\n",
    "    # list of Data structures (one for each subject)\n",
    "    train_graphs = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        g = array_to_graph(X_train[i], y_train[i])\n",
    "        train_graphs.append(g)\n",
    "        \n",
    "    test_graphs = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        g = array_to_graph(X_test[i], y_test[i])\n",
    "        test_graphs.append(g)\n",
    "\n",
    "    # create the model\n",
    "    model = GCN()\n",
    "    model = model.to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # train function\n",
    "    def train():\n",
    "        model.train()\n",
    "        \n",
    "        train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n",
    "\n",
    "        loss_all = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            label = batch.y\n",
    "            label = F.one_hot(label, num_classes=2)\n",
    "            label = label.type(torch.FloatTensor)\n",
    "            label = label.to(device)\n",
    "            loss = loss_fn(output, label)\n",
    "            loss.backward()\n",
    "            loss_all += batch.num_graphs * loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        return loss_all / len(train_graphs)\n",
    "\n",
    "    # train for N epochs\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        loss_value = train()\n",
    "        print(\"Train loss at epoch {}: {:.4f}\".format(epoch + 1, loss_value))\n",
    "\n",
    "    # test phase \n",
    "    test_loader = DataLoader(test_graphs, batch_size=len(test_graphs), shuffle=False)\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        test_preds = F.softmax(model(batch), dim=1).detach().numpy()\n",
    "    \n",
    "    test_preds = test_preds[:, 1]\n",
    "    preds[test_index] = test_preds\n",
    "    \n",
    "    auc_roc = roc_auc_score(y_test, test_preds)\n",
    "    print(\"Test AUC: {:.2f}\".format(auc_roc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
